{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Trainers\n",
    "\n",
    "In this tutorial, we demonstrate how to extend a TorchGeo [\"trainer class\"](https://torchgeo.readthedocs.io/en/latest/api/trainers.html). In TorchGeo there exist several trainer classes that are pre-made PyTorch Lightning Modules designed to allow for the easy training of models on semantic segmentation, classification, change detection, etc. tasks using TorchGeo's [prebuild DataModules](https://torchgeo.readthedocs.io/en/latest/api/datamodules.html). While the trainers aim to provide sensible defaults and customization options for common tasks, they will not be able to cover all situations (e.g. researchers will likely want to implement and use their own architectures, loss functions, optimizers, etc. in the training routine). If you run into such a situation, then you can simply extend the trainer class you are interested in, and write custom logic to override the default functionality.\n",
    "\n",
    "This tutorial shows how to do exactly this to customize a learning rate schedule, logging, and model checkpointing for a semantic segmentation task using the [LandCoverAI](https://landcover.ai.linuxpolska.com/) dataset.\n",
    "\n",
    "It's recommended to run this notebook on Google Colab if you don't have your own GPU. Click the \"Open in Colab\" button above to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As always, we install TorchGeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchgeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Next, we import TorchGeo and any other libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchgeo.trainers import SemanticSegmentationTask\n",
    "from torchgeo.datamodules import LandCoverAIDataModule\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    Accuracy,\n",
    "    FBetaScore,\n",
    "    Precision,\n",
    "    Recall,\n",
    "    JaccardIndex,\n",
    ")\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Get rid of the pesky warnings raised by kornia\n",
    "# UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SemanticSegmentationTask\n",
    "\n",
    "Now, we create a `CustomSemanticSegmentationTask` class that inhierits from `SemanticSegmentationTask` and that overrides a few methods:\n",
    "- `__init__`: We add two new parameters `tmax` and `eta_min` to control the learning rate scheduler\n",
    "- `configure_optimizers`: We use the `CosineAnnealingLR` learning rate scheduler instead of the default `ReduceLROnPlateau`\n",
    "- `configure_metrics`: We add a \"MeanIou\" metric (what we will use to evaluate the model's performance) and a variety of other classification metrics\n",
    "- `configure_callbacks`: We demonstrate how to stack `ModelCheckpoint` callbacks to save the best checkpoint as well as periodic checkpoints\n",
    "- `on_train_epoch_start`: We log the learning rate at the start of each epoch so we can easily see how it decays over a training run\n",
    "\n",
    "Overall these demonstrate how to customize the training routine to investigate specific research questions (e.g. of the scheduler on test performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSemanticSegmentationTask(SemanticSegmentationTask):\n",
    "\n",
    "    # any keywords we add here between *args and **kwargs will be found in self.hparams\n",
    "    def __init__(self, *args, tmax=50, eta_min=1e-6, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)  # pass args and kwargs to the parent class\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> \"lightning.pytorch.utilities.types.OptimizerLRSchedulerConfig\":\n",
    "        \"\"\"Initialize the optimizer and learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "            Optimizer and learning rate scheduler.\n",
    "        \"\"\"\n",
    "        tmax: int = self.hparams[\"tmax\"]\n",
    "        eta_min: float = self.hparams[\"eta_min\"]\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams[\"lr\"])\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=tmax, eta_min=eta_min)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": self.monitor},\n",
    "        }\n",
    "\n",
    "    def configure_metrics(self) -> None:\n",
    "        \"\"\"Initialize the performance metrics.\"\"\"\n",
    "        num_classes: int = self.hparams[\"num_classes\"]\n",
    "\n",
    "        self.train_metrics = MetricCollection(\n",
    "            {\n",
    "                \"OverallAccuracy\": Accuracy(\n",
    "                    task=\"multiclass\", num_classes=num_classes, average=\"micro\"\n",
    "                ),\n",
    "                \"OverallPrecision\": Precision(\n",
    "                    task=\"multiclass\", num_classes=num_classes, average=\"micro\"\n",
    "                ),\n",
    "                \"OverallRecall\": Recall(\n",
    "                    task=\"multiclass\", num_classes=num_classes, average=\"micro\"\n",
    "                ),\n",
    "                \"OverallF1Score\": FBetaScore(\n",
    "                    task=\"multiclass\",\n",
    "                    num_classes=num_classes,\n",
    "                    beta=1.0,\n",
    "                    average=\"micro\",\n",
    "                ),\n",
    "                \"MeanIoU\": JaccardIndex(\n",
    "                    num_classes=num_classes, task=\"multiclass\", average=\"macro\"\n",
    "                ),\n",
    "            },\n",
    "            prefix=\"train_\",\n",
    "        )\n",
    "        self.val_metrics = self.train_metrics.clone(prefix=\"val_\")\n",
    "        self.test_metrics = self.train_metrics.clone(prefix=\"test_\")\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        \"\"\"Initialize callbacks for saving the best and latest models.\n",
    "\n",
    "        Returns:\n",
    "            List of callbacks to apply.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            ModelCheckpoint(every_n_epochs=50, save_top_k=-1),\n",
    "            ModelCheckpoint(monitor=self.monitor, mode=self.mode, save_top_k=5),\n",
    "        ]\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        \"\"\"Log the learning rate at the start of each training epoch.\"\"\"\n",
    "        lr = self.optimizers().param_groups[0][\"lr\"]\n",
    "        self.logger.experiment.add_scalar(\"lr\", lr, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "The remainder of the turial is straightforward and follows the typical [PyTorch Lightning](https://lightning.ai/) training routine. We instantiate a `DataModule` for the LandCover.AI dataset, instantiate a `CustomSemanticSegmentationTask` with a U-Net and ResNet-50 backbone, then train the model using a Lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = LandCoverAIDataModule(\n",
    "    root=\"data/\",\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = CustomSemanticSegmentationTask(\n",
    "    model=\"unet\",\n",
    "    backbone=\"resnet50\",\n",
    "    weights=True,\n",
    "    in_channels=3,\n",
    "    num_classes=6,\n",
    "    loss=\"ce\",\n",
    "    lr=1e-3,\n",
    "    tmax=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"backbone\":        resnet50\n",
       "\"class_weights\":   None\n",
       "\"eta_min\":         1e-06\n",
       "\"freeze_backbone\": False\n",
       "\"freeze_decoder\":  False\n",
       "\"ignore\":          weights\n",
       "\"ignore_index\":    None\n",
       "\"in_channels\":     3\n",
       "\"loss\":            ce\n",
       "\"lr\":              0.001\n",
       "\"model\":           unet\n",
       "\"num_classes\":     6\n",
       "\"num_filters\":     3\n",
       "\"patience\":        10\n",
       "\"tmax\":            50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate that the task's hyperparameters are as expected\n",
    "task.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=accelerator, devices=[0], min_epochs=150, max_epochs=300, log_every_n_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://landcover.ai.linuxpolska.com/download/landcover.ai.v1.zip to data/landcover.ai.v1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1538212277/1538212277 [01:25<00:00, 17913845.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed M-33-20-D-c-4-2 1/41\n",
      "Processed M-33-20-D-d-3-3 2/41\n",
      "Processed M-33-32-B-b-4-4 3/41\n",
      "Processed M-33-48-A-c-4-4 4/41\n",
      "Processed M-33-7-A-d-2-3 5/41\n",
      "Processed M-33-7-A-d-3-2 6/41\n",
      "Processed M-34-32-B-a-4-3 7/41\n",
      "Processed M-34-32-B-b-1-3 8/41\n",
      "Processed M-34-5-D-d-4-2 9/41\n",
      "Processed M-34-51-C-b-2-1 10/41\n",
      "Processed M-34-51-C-d-4-1 11/41\n",
      "Processed M-34-55-B-b-4-1 12/41\n",
      "Processed M-34-56-A-b-1-4 13/41\n",
      "Processed M-34-6-A-d-2-2 14/41\n",
      "Processed M-34-65-D-a-4-4 15/41\n",
      "Processed M-34-65-D-c-4-2 16/41\n",
      "Processed M-34-65-D-d-4-1 17/41\n",
      "Processed M-34-68-B-a-1-3 18/41\n",
      "Processed M-34-77-B-c-2-3 19/41\n",
      "Processed N-33-104-A-c-1-1 20/41\n",
      "Processed N-33-119-C-c-3-3 21/41\n",
      "Processed N-33-130-A-d-3-3 22/41\n",
      "Processed N-33-130-A-d-4-4 23/41\n",
      "Processed N-33-139-C-d-2-2 24/41\n",
      "Processed N-33-139-C-d-2-4 25/41\n",
      "Processed N-33-139-D-c-1-3 26/41\n",
      "Processed N-33-60-D-c-4-2 27/41\n",
      "Processed N-33-60-D-d-1-2 28/41\n",
      "Processed N-33-96-D-d-1-1 29/41\n",
      "Processed N-34-106-A-b-3-4 30/41\n",
      "Processed N-34-106-A-c-1-3 31/41\n",
      "Processed N-34-140-A-b-3-2 32/41\n",
      "Processed N-34-140-A-b-4-2 33/41\n",
      "Processed N-34-140-A-d-3-4 34/41\n",
      "Processed N-34-140-A-d-4-2 35/41\n",
      "Processed N-34-61-B-a-1-1 36/41\n",
      "Processed N-34-66-C-c-4-3 37/41\n",
      "Processed N-34-77-A-b-1-4 38/41\n",
      "Processed N-34-94-A-b-2-4 39/41\n",
      "Processed N-34-97-C-b-1-2 40/41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed N-34-97-D-c-2-4 41/41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | CrossEntropyLoss | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | test_metrics  | MetricCollection | 0     \n",
      "4 | model         | Unet             | 32.5 M\n",
      "---------------------------------------------------\n",
      "32.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.5 M    Total params\n",
      "130.087   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00af14780e004ce69b89e436b7b95606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1439daa182f4d27b5194609bd194d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b547aad511247b1b0e88f79fbfa7e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calebrobinson/.conda/envs/geo/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(task, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n",
    "\n",
    "Finally, we test the model on the test set and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SemanticSegmentationTask.__init__() got an unexpected keyword argument 'ignore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If you are starting from a checkpoint, run this cell\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[43mCustomSemanticSegmentationTask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlightning_logs/version_3/checkpoints/epoch=0-step=117.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1561\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1482\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1488\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1561\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:89\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 89\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:156\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 156\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mCustomSemanticSegmentationTask.__init__\u001b[0;34m(self, tmax, eta_min, *args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, tmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: SemanticSegmentationTask.__init__() got an unexpected keyword argument 'ignore'"
     ]
    }
   ],
   "source": [
    "# If you are starting from a checkpoint, run this cell\n",
    "task = CustomSemanticSegmentationTask.load_from_checkpoint(\"lightning_logs/version_3/checkpoints/epoch=0-step=117.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(task, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
